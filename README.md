# On the exploration-exploitation trade off using deep recurrent reinforcement learning

**Authors:** Valentina Zangirolami and Matteo Borrotti

## **Description**
Double Dueling Deep Recurrent Q-Learning on Airsim simulator (Airsim NH environment).
<video src="https://user-images.githubusercontent.com/78240304/149147549-29936bd7-f629-4b66-a125-ddcd50443bcb.mp4" width="224">
## **Prerequisites**
  * Python 3.7.6 
  * Tensorflow 2.5.0
  * Tornado 4.5.3
  * OpenCV 4.5.2.54
  * OpenAI Gym 0.18.3
  * Airsim 1.5.0
  
## **Hardware**
  * 2 GPU Tesla M60 with 8 Gb

## **References**
[1] Gimelfarb, M., S. Sanner, and C.-G. Lee, 2020: *Îµ-BMC: A Bayesian Ensemble Approach to Epsilon-Greedy Exploration in Model-Free Reinforcement Learning*. CoRR 

[2] Juliani A., 2016: *Simple Reinforcement Learning with Tensorflow Part 6: Partial Observability and Deep Recurrent Q-Networks*. URL: https://github.com/awjuliani/DeepRL-Agents

[3] Riboni, A., A. Candelieri, and M. Borrotti, 2021: *Deep Autonomous Agents comparison for Self-Driving Cars*. Proceedings of The 7th International Conference on Machine Learning, Optimization and Big Data - LOD 
  
[4] *Welcome to AirSim*, https://microsoft.github.io/AirSim/
## **Acknowledgements**
I acknowledge Data Science Lab of Department of Economics, Management and Statistics (DEMS) of University of Milan-Bicocca for providing a virtual machine.

